{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3823efd-0856-4081-a276-683aaa669406",
   "metadata": {},
   "source": [
    "We‚Äôll use the IMDb movie review sentiment analysis dataset, which classifies reviews as Positive (1) or Negative (0) ‚Äî perfect for learning how a GRU (Gated Recurrent Unit) works for text classification.\n",
    "\n",
    "üé¨ GRU Sentiment Analysis ‚Äî Complete Project\n",
    "üì¶ Step 1: Import Libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "üß† Explanation:\n",
    "\n",
    "numpy ‚Üí helps with numerical operations.\n",
    "\n",
    "imdb ‚Üí built-in IMDb dataset from Keras.\n",
    "\n",
    "pad_sequences ‚Üí ensures all reviews have the same length.\n",
    "\n",
    "Sequential ‚Üí lets us build a model layer by layer.\n",
    "\n",
    "Embedding ‚Üí converts words (integers) into dense vectors for the GRU.\n",
    "\n",
    "GRU ‚Üí the core Recurrent layer for text understanding.\n",
    "\n",
    "Dense, Dropout ‚Üí used for classification and regularization.\n",
    "\n",
    "Adam ‚Üí optimizer for training.\n",
    "\n",
    "üìö Step 2: Load and Prepare the Dataset\n",
    "# Limit vocabulary to 10,000 most frequent words\n",
    "vocab_size = 10000\n",
    "maxlen = 200  # each review cut or padded to 200 words\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Pad sequences so they all have the same length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "üß† Explanation:\n",
    "\n",
    "IMDb dataset comes tokenized (each word replaced by an integer).\n",
    "\n",
    "We only keep the 10,000 most common words.\n",
    "\n",
    "Reviews have different lengths ‚Üí GRUs need equal-length inputs.\n",
    "\n",
    "pad_sequences pads or truncates reviews to 200 tokens each.\n",
    "\n",
    "üß† Step 3: Build the GRU Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128, input_length=maxlen),\n",
    "    GRU(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "üß† Explanation (Layer by Layer):\n",
    "Layer\tPurpose\n",
    "Embedding\tConverts each word index (1‚Äì10000) into a 128-dimensional dense vector (so the model learns word meanings).\n",
    "GRU(64)\tReads the sequence word by word, learning context and dependencies (64 hidden units). The dropout terms help prevent overfitting.\n",
    "Dense(64, relu)\tAdds a fully connected layer to combine GRU features.\n",
    "Dropout(0.3)\tRandomly turns off 30% of neurons during training ‚Äî improves generalization.\n",
    "Dense(1, sigmoid)\tOutput layer: predicts a probability between 0 and 1 (Positive or Negative).\n",
    "‚öôÔ∏è Step 4: Compile the Model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "üß† Explanation:\n",
    "\n",
    "Loss: binary_crossentropy ‚Äî used for 2-class problems (0 or 1).\n",
    "\n",
    "Optimizer: Adam ‚Äî adaptive gradient descent.\n",
    "\n",
    "Metrics: Accuracy ‚Äî shows how many predictions are correct.\n",
    "\n",
    "üöÄ Step 5: Train the Model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "üß† Explanation:\n",
    "\n",
    "epochs=3 ‚Üí how many times the model goes through all training data.\n",
    "\n",
    "batch_size=64 ‚Üí number of samples per training step.\n",
    "\n",
    "validation_split=0.2 ‚Üí 20% of training data used to check performance after each epoch.\n",
    "\n",
    "verbose=1 ‚Üí prints progress bar.\n",
    "\n",
    "‚úÖ Output Example:\n",
    "\n",
    "Epoch 1/3\n",
    "313/313 [==============================] - 25s 69ms/step - loss: 0.4500 - accuracy: 0.7823 - val_loss: 0.3371 - val_accuracy: 0.8620\n",
    "...\n",
    "\n",
    "üìä Step 6: Evaluate the Model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "üß† Explanation:\n",
    "\n",
    "This tests the model on unseen data and prints accuracy ‚Äî ideally above 85%.\n",
    "\n",
    "üí¨ Step 7: Predict on New Reviews\n",
    "# Example: Predict a single review sentiment\n",
    "sample = x_test[10].reshape(1, maxlen)\n",
    "prediction = model.predict(sample)\n",
    "print(\"Predicted Sentiment:\", \"Positive\" if prediction[0][0] > 0.5 else \"Negative\")\n",
    "\n",
    "üß† Explanation:\n",
    "\n",
    "We take one review from test data.\n",
    "\n",
    "The GRU predicts a probability.\n",
    "\n",
    "0.5 = Positive, else Negative.\n",
    "\n",
    "üñºÔ∏è Step 8: Visualize Training Progress (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('GRU Model Accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "‚úÖ You‚Äôll see a nice curve showing accuracy improving over epochs.\n",
    "\n",
    "üß† Summary Table\n",
    "Step\tCode Section\tDescription\n",
    "1\tImport libraries\tLoad all Keras tools\n",
    "2\tLoad IMDb data\tPreprocess text sequences\n",
    "3\tBuild GRU model\tCreate GRU layers\n",
    "4\tCompile\tDefine loss, optimizer\n",
    "5\tTrain\tFit model to training data\n",
    "6\tEvaluate\tCheck performance on test data\n",
    "7\tPredict\tTest with new samples\n",
    "8\tVisualize\tShow accuracy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac3953-0c20-4212-b567-c7fc274d0b38",
   "metadata": {},
   "source": [
    "We limit vocabulary to 10k most frequent words.\n",
    "\n",
    "Each review is converted to a sequence of numbers (word indices).\n",
    "\n",
    "GRU needs equal-length sequences ‚Üí we pad them to 200.\n",
    "Embedding: Transforms words into 128-dimensional vectors.\n",
    "\n",
    "GRU(64): Learns word dependencies in the text sequence.\n",
    "\n",
    "Dense(64): Fully connected layer for learned features.\n",
    "\n",
    "Dropout(0.3): Prevents overfitting by randomly disabling neurons.\n",
    "\n",
    "Dense(1, sigmoid): Outputs probability for Positive (1) or Negative (0).\n",
    "Model trains for 3 epochs (complete passes through data).\n",
    "\n",
    "Uses 64 samples per batch.\n",
    "\n",
    "20% of training data is kept for validation.\n",
    "\n",
    "verbose=1 prints training progress.\n",
    "Evaluates how well the GRU generalizes to new data.\n",
    "\n",
    "Accuracy above 85% is common for this task.\n",
    "Takes a single review.\n",
    "\n",
    "Predicts probability ‚Üí 1 = Positive, 0 = Negative.\n",
    "Plots how accuracy improves each epoch.\n",
    "\n",
    "Helps check for overfitting or underfitting visually.\n",
    "Takes a single review.\n",
    "\n",
    "Predicts probability ‚Üí 1 = Positive, 0 = Negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af763ae-b749-4b07-b62f-25a72a9edd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò Sentiment Analysis using GRU (Gated Recurrent Units)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Import the required libraries\n",
    "# -------------------------------------------------------\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Load and preprocess the IMDb dataset\n",
    "# -------------------------------------------------------\n",
    "vocab_size = 10000      # Use top 10,000 most frequent words\n",
    "maxlen = 200            # Maximum review length (pad/truncate to 200)\n",
    "\n",
    "# Load IMDb data (already tokenized into integers)\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Pad all sequences to the same length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Build the GRU model\n",
    "# -------------------------------------------------------\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128, input_length=maxlen),\n",
    "    GRU(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary output\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Compile the model\n",
    "# -------------------------------------------------------\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Train the model\n",
    "# -------------------------------------------------------\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6Ô∏è‚É£ Evaluate model performance\n",
    "# -------------------------------------------------------\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(f\"\\n‚úÖ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7Ô∏è‚É£ Make a prediction on a sample review\n",
    "# -------------------------------------------------------\n",
    "sample = x_test[10].reshape(1, maxlen)\n",
    "prediction = model.predict(sample)\n",
    "print(\"\\nPredicted Sentiment:\", \"Positive\" if prediction[0][0] > 0.5 else \"Negative\")\n",
    "print(\"Raw prediction value:\", prediction[0][0])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8Ô∏è‚É£ Visualize accuracy progress\n",
    "# -------------------------------------------------------\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('GRU Model Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 9Ô∏è‚É£ Summary\n",
    "# -------------------------------------------------------\n",
    "print(\"\\nüß† SUMMARY:\")\n",
    "print(\"1. Loaded and preprocessed IMDb dataset\")\n",
    "print(\"2. Built GRU-based deep learning model\")\n",
    "print(\"3. Trained for 3 epochs with dropout to prevent overfitting\")\n",
    "print(\"4. Evaluated accuracy on test data\")\n",
    "print(\"5. Predicted new review sentiment successfully ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc1eb9f-c51c-4191-925b-f0403306d4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
