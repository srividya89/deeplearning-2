{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644997ad-c69e-414d-897a-ef222463281b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 3s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 64)           640000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               98816     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 738945 (2.82 MB)\n",
      "Trainable params: 738945 (2.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "313/313 [==============================] - 101s 314ms/step - loss: 0.4658 - accuracy: 0.7756 - val_loss: 0.3596 - val_accuracy: 0.8466\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 90s 288ms/step - loss: 0.2824 - accuracy: 0.8891 - val_loss: 0.3687 - val_accuracy: 0.8428\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 101s 322ms/step - loss: 0.2118 - accuracy: 0.9204 - val_loss: 0.3499 - val_accuracy: 0.8558\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 91s 292ms/step - loss: 0.1523 - accuracy: 0.9460 - val_loss: 0.3646 - val_accuracy: 0.8542\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 87s 279ms/step - loss: 0.1221 - accuracy: 0.9574 - val_loss: 0.3880 - val_accuracy: 0.8402\n",
      "782/782 [==============================] - 45s 57ms/step - loss: 0.3957 - accuracy: 0.8414\n",
      "Test Accuracy: 0.841\n",
      "1/1 [==============================] - 0s 383ms/step\n",
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# Import libraries  #RNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10000   # Only consider the top 10,000 words\n",
    "maxlen = 200         # Cut texts after 200 words\n",
    "embedding_dim = 64\n",
    "\n",
    "# Load IMDb dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Pad sequences (to make all sequences the same length)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate on test data\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "# Example prediction\n",
    "sample = x_test[0].reshape(1, -1)\n",
    "prediction = model.predict(sample)[0][0]\n",
    "print(\"Predicted Sentiment:\", \"Positive\" if prediction > 0.5 else \"Negative\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61311313-9347-4cf6-bc2e-2312ef257f17",
   "metadata": {},
   "source": [
    "Detailed explanation, line by line\n",
    "Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "Imports TensorFlow (the high-level ML library). We alias it tf because that’s standard. This gives access to Keras (tf.keras) and low-level TensorFlow APIs.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "Imports the Sequential model class from Keras. Sequential is a container for stacking layers in order (a linear stack).\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "Imports layer types used in the model:\n",
    "\n",
    "Embedding: maps integer word indices → dense vectors (learned embeddings).\n",
    "\n",
    "LSTM: Long Short-Term Memory recurrent layer (captures sequence patterns).\n",
    "\n",
    "Dense: fully connected (classification) layer.\n",
    "\n",
    "Dropout: regularization layer that randomly zeroes activations to reduce overfitting.\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "Utility to pad (or truncate) lists of token IDs so every input sequence has the same length (required for batching).\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "Imports Keras’s built-in IMDB reviews dataset (preprocessed as integer token indices).\n",
    "\n",
    "Hyperparameters / configuration\n",
    "\n",
    "vocab_size = 10000 # Only consider the top 10,000 words\n",
    "Use only the top 10k most frequent words in the dataset. Words outside this set are replaced with an \"out-of-vocab\" token. Limits vocabulary size and memory.\n",
    "\n",
    "maxlen = 200 # Cut texts after 200 words\n",
    "All sequences will be exactly 200 tokens long after padding/truncation. Longer reviews are truncated; shorter ones are padded.\n",
    "\n",
    "embedding_dim = 64\n",
    "Each word will be represented by a 64-dimensional embedding vector. This controls embedding layer output size and thus number of parameters.\n",
    "\n",
    "Load the dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "Loads IMDB data already converted to integer sequences.\n",
    "\n",
    "x_train and x_test are lists/arrays of integer lists: each review is a list of word indices (integers from 1..vocab_size-1; 0 is usually reserved for padding).\n",
    "\n",
    "y_train and y_test are binary labels: 0 for negative, 1 for positive.\n",
    "\n",
    "num_words=vocab_size keeps only top vocab_size words; rarer words are replaced by a reserved index.\n",
    "\n",
    "Shape note: x_train is a Python list of length ~25,000 (for IMDB). Each element is variable length before padding.\n",
    "\n",
    "Pad sequences\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "Pads or truncates every sequence in x_train so each becomes length maxlen. By default pad_sequences pads with zeros at the beginning (padding='pre') and truncates longer sequences from the start (truncating='pre'). You can change to padding='post' if you prefer trailing zeros.\n",
    "\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "Same as above for the test set.\n",
    "\n",
    "Shape after padding: x_train.shape == (num_examples, maxlen); with IMDB typically (25000, 200).\n",
    "\n",
    "Build the model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "Sequential([...])\n",
    "Create a sequential model and pass the ordered layers.\n",
    "\n",
    "Embedding(vocab_size, embedding_dim, input_length=maxlen)\n",
    "\n",
    "Input: integer sequences shaped (batch_size, maxlen) where each integer is in [0, vocab_size-1].\n",
    "\n",
    "Output: a 3D tensor shaped (batch_size, maxlen, embedding_dim) — each token replaced by a learned embedding_dim vector.\n",
    "\n",
    "vocab_size is the input dimension (size of the vocabulary + reserved indices). The embedding matrix size will be (vocab_size, embedding_dim) and is learned during training.\n",
    "\n",
    "LSTM(128, return_sequences=False)\n",
    "\n",
    "An LSTM layer with 128 hidden units (the dimensionality of the LSTM output/state).\n",
    "\n",
    "return_sequences=False (default) means the layer returns only the final hidden state for the whole sequence (shape (batch_size, 128)), not a sequence of outputs at each timestep.\n",
    "\n",
    "If you set return_sequences=True, the LSTM would return an output for each timestep (shape (batch_size, maxlen, 128)), useful for stacked RNNs or sequence-to-sequence tasks.\n",
    "\n",
    "Dropout(0.5)\n",
    "\n",
    "During training, randomly sets 50% of inputs to zero to reduce overfitting. Applied to the LSTM output before the Dense layer.\n",
    "\n",
    "Note: this is standard Dropout applied to layer activations. LSTMs also support recurrent_dropout inside the LSTM for recurrent connections.\n",
    "\n",
    "Dense(1, activation='sigmoid')\n",
    "\n",
    "Final classification layer with a single output neuron.\n",
    "\n",
    "sigmoid squashes outputs to (0,1), giving probability of the positive class. For binary classification, this and binary_crossentropy loss is standard.\n",
    "\n",
    "Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "loss='binary_crossentropy'\n",
    "\n",
    "Appropriate loss for binary classification with a single sigmoid output. It measures how close predicted probabilities are to true labels.\n",
    "\n",
    "optimizer='adam'\n",
    "\n",
    "Adam optimizer — adaptive learning rate method that works well out of the box.\n",
    "\n",
    "metrics=['accuracy']\n",
    "\n",
    "Track accuracy during training and evaluation.\n",
    "\n",
    "Model summary\n",
    "\n",
    "model.summary()\n",
    "Prints a table with each layer, output shapes, and number of trainable parameters. Useful to verify dimension flow and parameter counts.\n",
    "\n",
    "Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "x_train, y_train — training data.\n",
    "\n",
    "epochs=5 — run through the entire training set 5 times. Increase for better fit; watch for overfitting.\n",
    "\n",
    "batch_size=64 — number of samples per gradient update. Smaller batches give noisier but sometimes better generalization; larger batches are faster on GPUs but need more memory.\n",
    "\n",
    "validation_split=0.2 — hold out 20% of the training data for validation (Keras will split the last 20% of x_train and y_train). The model evaluates loss/accuracy on this split after each epoch, which helps monitor overfitting.\n",
    "\n",
    "history stores training/validation loss & metrics per epoch (useful to plot learning curves).\n",
    "\n",
    "Notes:\n",
    "\n",
    "If your dataset is already pre-shuffled, validation_split is fine. For some datasets you'd want to use a dedicated validation_data=(x_val, y_val) instead of splitting.\n",
    "\n",
    "Training on CPU may be slow — GPU accelerates RNN training significantly.\n",
    "\n",
    "Evaluate on test data\n",
    "\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "Runs the trained model on the held-out test set and returns loss and the metrics defined during compile (accuracy here). Gives an unbiased estimate of final performance.\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "Prints test accuracy to 3 decimal places.\n",
    "\n",
    "Single sample prediction\n",
    "sample = x_test[0].reshape(1, -1)\n",
    "prediction = model.predict(sample)[0][0]\n",
    "print(\"Predicted Sentiment:\", \"Positive\" if prediction > 0.5 else \"Negative\")\n",
    "\n",
    "\n",
    "x_test[0] selects the first padded test review — shape (maxlen,).\n",
    "\n",
    ".reshape(1, -1) changes it to (1, maxlen) to form a batch of size 1, because model.predict expects a batch dimension.\n",
    "\n",
    "model.predict(sample) returns an array of shape (1, 1) (batch_size, output_dim). [0][0] extracts the scalar probability.\n",
    "\n",
    "The if prediction > 0.5 threshold converts probability into a binary class (0.5 is the common default). You can change threshold depending on precision/recall preferences.\n",
    "\n",
    "Extra tips, common changes, and gotchas\n",
    "\n",
    "Padding direction: By default pad_sequences uses padding='pre'. For many text tasks padding='post' is convenient because the start of sequence aligns across examples; use pad_sequences(..., padding='post', truncating='post') if you prefer.\n",
    "\n",
    "Embedding index 0 = padding: Keras’s pad_sequences uses 0 for padding. If your dataset uses index 0 for a real token, adjust accordingly. With imdb.load_data() index 0 is reserved.\n",
    "\n",
    "Return_sequences True: If you want to stack another RNN after this LSTM (e.g., a second LSTM), set return_sequences=True on the first LSTM.\n",
    "\n",
    "Bidirectional LSTM: For better performance on many text tasks, wrap the LSTM with tf.keras.layers.Bidirectional(...), e.g.\n",
    "Bidirectional(LSTM(128)) — doubles effective context (forward + backward).\n",
    "\n",
    "Regularization: You can add recurrent_dropout=0.2 to the LSTM to regularize recurrent connections or add L2 penalties.\n",
    "\n",
    "Changing model capacity: Increase/decrease LSTM(128) units or embedding_dim to change model capacity. More units = more expressive but more prone to overfitting.\n",
    "\n",
    "Training length: More epochs often help but monitor validation loss — if validation loss rises while training loss falls, you’re overfitting; use early stopping:\n",
    "tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True).\n",
    "\n",
    "Performance: LSTMs are slower than simpler RNNs and slower than Transformer architectures for long sequences. For production/performance consider GRU (faster) or Transformer-based models for large tasks.\n",
    "\n",
    "If you want, I can:\n",
    "\n",
    "Annotate the code inline with comments instead of separate explanations.\n",
    "\n",
    "Convert this to a GRU or SimpleRNN.\n",
    "\n",
    "Add preprocessing (like mapping raw text → tokens) so you can use your own dataset (not just IMDB).\n",
    "\n",
    "Add callbacks (ModelCheckpoint, EarlyStopping) and plotting of history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5445b2-1ebc-4793-bc84-b7ebd2dcb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "“First, we import TensorFlow — that’s the main deep learning library we’ll use.\n",
    "Inside TensorFlow, we use Keras — the high-level API — to easily build our model.\n",
    "\n",
    "Sequential means we’ll stack layers one after another.\n",
    "\n",
    "Embedding converts words into numerical vectors.\n",
    "\n",
    "LSTM is our main RNN layer that can remember long-term patterns.\n",
    "\n",
    "Dense and Dropout handle final classification and regularization.\n",
    "\n",
    "Finally, we import imdb, a built-in dataset of movie reviews.”\n",
    "\n",
    "“Here, Keras automatically downloads the IMDb dataset.\n",
    "It comes preprocessed — every review is already converted into a list of numbers.\n",
    "For example, the word ‘amazing’ might be number 52, ‘movie’ might be 37, and so on.”\n",
    "\n",
    "🧠 (Visual idea: Show a sentence like “The movie was amazing” → [37, 24, 15, 52].)\n",
    "\n",
    "“Each review is a different length — some are 50 words, some are 300.\n",
    "But neural networks need fixed-length inputs.\n",
    "So pad_sequences adds zeros at the beginning of shorter reviews,\n",
    "and cuts off extra words if they’re too long — all reviews become 200 words long.”\n",
    "\n",
    "🟦 (Visual idea: Rows of sequences with 0-padding on the left to make all equal length.)\n",
    "\n",
    "Now we build the neural network.\n",
    "\n",
    "The Embedding layer takes those numbers (word IDs) and turns them into word vectors — 64 numbers per word, learned automatically during training.\n",
    "\n",
    "The LSTM layer has 128 memory cells. It reads the sequence one word at a time and tries to understand the meaning of the whole sentence.\n",
    "\n",
    "The Dropout layer randomly turns off half the neurons while training — that prevents overfitting.\n",
    "\n",
    "Finally, the Dense layer outputs a single number between 0 and 1, using a sigmoid activation — 0 means negative review, 1 means positive.”\n",
    "\n",
    "🎥 (Visual idea: Show arrows connecting “Embedding → LSTM → Dropout → Dense → output”.)\n",
    "\n",
    "\n",
    "“Now we tell the model how to learn.\n",
    "\n",
    "We use binary_crossentropy since there are only two classes: positive or negative.\n",
    "\n",
    "adam optimizer automatically adjusts learning speed.\n",
    "\n",
    "And we track accuracy to see how well it performs.”\n",
    "\n",
    "This prints a summary of each layer — the shapes, and how many trainable parameters there are.\n",
    "It’s a good sanity check before training.”\n",
    "\n",
    "“Let’s train the model!\n",
    "We’ll train for 5 rounds — called epochs.\n",
    "Each round, the model sees all 25,000 reviews and tries to minimize the loss.\n",
    "The data is divided into batches of 64 samples each.\n",
    "\n",
    "We also use 20% of the data for validation — that helps us check if the model is overfitting.”\n",
    "\n",
    "📈 (Visual idea: Loss curve and accuracy curve rising during epochs.)\n",
    "\n",
    "    “After training, we test the model on completely unseen reviews.\n",
    "The test accuracy tells us how well it generalizes — ideally above 80%.”\n",
    "\n",
    "    “Finally, we test one example manually.\n",
    "We reshape it into a batch of 1, and the model predicts a number between 0 and 1.\n",
    "If it’s greater than 0.5, it’s positive — otherwise, negative.\n",
    "Simple and powerful!”\n",
    "\n",
    "🏁 Wrap-Up\n",
    "\n",
    "🎙️ Voiceover:\n",
    "\n",
    "“So that’s how an LSTM-based RNN reads movie reviews and predicts their sentiment!\n",
    "\n",
    "Remember the flow:\n",
    "Text → Tokenization → Padding → Embedding → LSTM → Dense Output.\n",
    "\n",
    "You can use this same idea for any kind of sequence data — like chat messages, stock prices, or even music notes.”\n",
    "\n",
    "    “Let’s start by importing the libraries.\n",
    "TensorFlow gives us access to Keras — the easy-to-use high-level API for deep learning.\n",
    "\n",
    "Sequential helps us stack layers in order.\n",
    "\n",
    "Embedding converts words to numerical vectors.\n",
    "\n",
    "LSTM is our main RNN layer that captures sequence patterns.\n",
    "\n",
    "Dense and Dropout are for the output and regularization.\n",
    "\n",
    "And the imdb dataset contains 25,000 movie reviews ready for sentiment analysis.”\n",
    "\n",
    "\n",
    "    “Before building the model, we define a few hyperparameters.\n",
    "\n",
    "vocab_size = 10,000: We’ll only use the 10,000 most common words in English reviews.\n",
    "\n",
    "maxlen = 200: Each review will be trimmed or padded to exactly 200 words.\n",
    "\n",
    "embedding_dim = 64: Every word will be represented by a 64-dimensional learned vector.”\n",
    "\n",
    "    “Keras makes loading the IMDb dataset extremely simple.\n",
    "Each review is already converted into a list of integers — where each number represents a specific word.\n",
    "\n",
    "Here, we print out how many reviews we have and inspect one example.\n",
    "Notice that the words aren’t plain English yet — just numbers that point to words in the vocabulary.”\n",
    "\n",
    "        “Each review has a different length, but neural networks require fixed-size inputs.\n",
    "\n",
    "We use pad_sequences to make all reviews 200 words long.\n",
    "Shorter reviews are padded with zeros at the beginning, longer ones are truncated.\n",
    "\n",
    "The shape printed here confirms each review now has exactly 200 tokens.”\n",
    "        “Now, let’s build our model!\n",
    "\n",
    "We start with an Embedding layer — this layer learns a dense vector for every word.\n",
    "\n",
    "Next is the LSTM layer with 128 memory cells.\n",
    "LSTM stands for Long Short-Term Memory.\n",
    "It reads the sequence word by word and remembers important information from earlier words.\n",
    "\n",
    "return_sequences=False means we only want the final output — perfect for classification.\n",
    "\n",
    "After that, a Dropout layer randomly turns off 50% of neurons during training to prevent overfitting.\n",
    "\n",
    "Finally, a Dense layer with a sigmoid activation gives a probability between 0 and 1 — predicting whether the review is positive or negative.\n",
    "\n",
    "When we call model.summary(), we can see each layer’s shape and the number of trainable parameters.”\n",
    "    Before training, we compile the model.\n",
    "\n",
    "binary_crossentropy is the loss function for two-class problems.\n",
    "\n",
    "The adam optimizer adapts the learning rate automatically.\n",
    "\n",
    "And we’ll track accuracy as our metric.”\n",
    "    “Now we’re ready to train!\n",
    "We’ll run for five epochs, meaning the model will see the entire training set five times.\n",
    "\n",
    "Each batch contains 64 samples, and we hold out 20% of the data for validation.\n",
    "\n",
    "As it trains, you’ll see loss and accuracy improving with each epoch.\n",
    "\n",
    "Usually, by the 4th or 5th epoch, accuracy reaches around 85% on validation data — which is quite good for this small network.”\n",
    "\n",
    "Visual cue: Simulate training output lines appearing, highlight accuracy increasing.\n",
    "\n",
    "    “Once training finishes, we test the model on completely unseen data — the 25,000 test reviews.\n",
    "\n",
    "This gives us a realistic measure of how well our model generalizes.\n",
    "A test accuracy above 80% means it’s doing a solid job understanding review sentiment.”\n",
    "    “Let’s try predicting one review manually.\n",
    "\n",
    "We take the first example from our test set, reshape it into a batch of one, and feed it to the model.\n",
    "\n",
    "The model outputs a number between 0 and 1 — if it’s above 0.5, we call it positive; otherwise negative.\n",
    "\n",
    "And that’s how our neural network interprets a piece of text!”\n",
    "\n",
    "[10:15 – 11:30] Concept Recap\n",
    "\n",
    "Visual: Slide/markdown cell summarizing the flow:\n",
    "\n",
    "Text → Tokenization → Padding → Embedding → LSTM → Dense → Sentiment\n",
    "\n",
    "\n",
    "Voiceover:\n",
    "\n",
    "“Let’s quickly review the workflow:\n",
    "\n",
    "First, we convert text into tokens.\n",
    "\n",
    "Then we pad to equal length.\n",
    "\n",
    "The Embedding layer learns relationships between words.\n",
    "\n",
    "The LSTM processes them sequentially to find meaning.\n",
    "\n",
    "Finally, the Dense layer makes a binary prediction.\n",
    "\n",
    "That’s the full pipeline of text sentiment classification using RNNs.”\n",
    "\n",
    "[11:30 – 12:00] Closing and Next Steps\n",
    "\n",
    "Visual: Outro markdown cell:\n",
    "\n",
    "✅ You learned:\n",
    "- How to build an RNN (LSTM) with Keras\n",
    "- How to train on IMDb movie reviews\n",
    "- How to test and predict sentiment\n",
    "Next step: try GRU or bidirectional LSTM for better accuracy!\n",
    "\n",
    "\n",
    "Voiceover:\n",
    "\n",
    "“And that wraps up our tutorial!\n",
    "You’ve learned how an RNN processes sequential text data, how embeddings work, and how to train your own sentiment classifier.\n",
    "\n",
    "Next, you can experiment with GRUs or Bidirectional LSTMs for even better results.\n",
    "\n",
    "Thanks for watching — and happy coding!”\n",
    "\n",
    "Visual: Fade-out with “Subscribe for more Deep Learning Tutorials”.\n",
    "\n",
    "🎧 Optional: Voice-Over Production Settings\n",
    "\n",
    "To create the actual video:\n",
    "\n",
    "Tool suggestion: HeyGen\n",
    " or Pika Labs\n",
    ".\n",
    "\n",
    "Voice style: Female or male, calm “teacher” tone.\n",
    "\n",
    "Script speed: ~120–130 words per minute → 12-minute total length.\n",
    "\n",
    "Background: Plain Jupyter Notebook screen capture; use subtle zooms on code blocks and highlight important lines with a yellow rectangle overlay.\n",
    "\n",
    "Add light background music (low volume instrumental) for a polished effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d92ca036-d787-4921-891c-fbe754cebe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "313/313 [==============================] - 30s 89ms/step - loss: 0.6590 - accuracy: 0.5937 - val_loss: 0.4993 - val_accuracy: 0.7648\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 26s 83ms/step - loss: 0.4085 - accuracy: 0.8248 - val_loss: 0.4961 - val_accuracy: 0.7572\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 27s 87ms/step - loss: 0.3305 - accuracy: 0.8640 - val_loss: 0.3932 - val_accuracy: 0.8466\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.4142 - accuracy: 0.8354\n",
      "Simple RNN Test Accuracy: 0.835\n",
      "Simple RNN: 0.847\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "vocab_size = 10000\n",
    "maxlen = 200\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "simple_rnn_model = Sequential([\n",
    "    Embedding(vocab_size, 64, input_length=maxlen),\n",
    "    SimpleRNN(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "simple_rnn_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_rnn = simple_rnn_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")\n",
    "loss, acc = simple_rnn_model.evaluate(x_test, y_test)\n",
    "print(f\"Simple RNN Test Accuracy: {acc:.3f}\")\n",
    "print(\"Simple RNN:\", round(history_rnn.history['val_accuracy'][-1], 3))\n",
    "# Example prediction\n",
    "sample1= x_test[0].reshape(1, -1)\n",
    "prediction = simple_rnn_model.predict(sample1)[0][0]\n",
    "print(\"Predicted Sentiment:\", \"Positive\" if prediction > 0.5 else \"Negative\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea005aab-f0ac-429f-b516-b76e66ba5f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras.api._v2.keras.datasets.imdb' from 'C:\\\\Users\\\\Welcome\\\\anaconda3\\\\envs\\\\mycorrectenv\\\\lib\\\\site-packages\\\\keras\\\\api\\\\_v2\\\\keras\\\\datasets\\\\imdb\\\\__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6092ab6-1fb0-4e5a-a0dd-8653c694bea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "#to see the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3a6b86-e846-439c-aad5-5a08ba915c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_HOME'] = 'D:/keras_datasets'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "261b4ea2-7dab-46c7-9db3-6b64a0ef5feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 25000 Test: 25000\n",
      "Example tokens: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25] Label: 1\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "   8192/1641221 [..............................] - ETA: 0s"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\mycorrectenv\\lib\\site-packages\\keras\\src\\utils\\data_utils.py:88\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunk_read(response, reporthook\u001b[38;5;241m=\u001b[39mreporthook):\n\u001b[1;32m---> 88\u001b[0m     \u001b[43mfd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(x_train), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(x_test))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x_train[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m20\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_train[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 12\u001b[0m word_index \u001b[38;5;241m=\u001b[39m \u001b[43mimdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_word_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m word_index \u001b[38;5;241m=\u001b[39m {k: (v \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m word_index\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     14\u001b[0m word_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m\"\u001b[39m], word_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<START>\u001b[39m\u001b[38;5;124m\"\u001b[39m], word_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m\"\u001b[39m], word_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<UNUSED>\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mycorrectenv\\lib\\site-packages\\keras\\src\\datasets\\imdb.py:211\u001b[0m, in \u001b[0;36mget_word_index\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves a dict mapping words to their index in the IMDB dataset.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    208\u001b[0m origin_folder \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://storage.googleapis.com/tensorflow/tf-keras-datasets/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    210\u001b[0m )\n\u001b[1;32m--> 211\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimdb_word_index.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbfafd718b763782e994055a2d397834f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mycorrectenv\\lib\\site-packages\\keras\\src\\utils\\data_utils.py:347\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m         \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLProgbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmsg))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mycorrectenv\\lib\\site-packages\\keras\\src\\utils\\data_utils.py:86\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     85\u001b[0m response \u001b[38;5;241m=\u001b[39m urlopen(url, data)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunk_read(response, reporthook\u001b[38;5;241m=\u001b[39mreporthook):\n\u001b[0;32m     88\u001b[0m         fd\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 10000\n",
    "maxlen = 200\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "print(\"Train:\", len(x_train), \"Test:\", len(x_test))\n",
    "print(\"Example tokens:\", x_train[0][:20], \"Label:\", y_train[0])\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"], word_index[\"<START>\"], word_index[\"<UNK>\"], word_index[\"<UNUSED>\"] = 0, 1, 2, 3\n",
    "reverse_word_index = {v:k for k,v in word_index.items()}\n",
    "\n",
    "def decode_review(encoded):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in encoded])\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Review\", i+1)\n",
    "    print(decode_review(x_train[i][:100]))\n",
    "    print(\"Sentiment:\", \"Positive\" if y_train[i]==1 else \"Negative\")\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test  = pad_sequences(x_test,  maxlen=maxlen)\n",
    "print(\"Padded shapes:\", x_train.shape, x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae89712-5c75-40c9-91ff-5f11e39f5b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad526f3d-199f-4279-aab1-bfe292eb029e",
   "metadata": {},
   "source": [
    "Excellent — you caught a very common but important issue 💡\n",
    "\n",
    "The error:\n",
    "\n",
    "OSError: [Errno 28] No space left on device\n",
    "\n",
    "\n",
    "means that your disk or the temporary folder TensorFlow/Keras uses to download datasets is full — not that your code is wrong.\n",
    "\n",
    "Let’s break this down clearly 👇\n",
    "\n",
    "🧩 1️⃣ Why This Happens\n",
    "\n",
    "When you call:\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "\n",
    "Keras tries to download a small JSON file:\n",
    "\n",
    "https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
    "\n",
    "\n",
    "and store it in your local cache folder, usually:\n",
    "\n",
    "~/.keras/datasets/\n",
    "\n",
    "\n",
    "or\n",
    "on Windows:\n",
    "\n",
    "C:\\Users\\<YourName>\\.keras\\datasets\\\n",
    "\n",
    "\n",
    "If that folder (or your disk) is full, Python cannot write the file → “No space left on device”.\n",
    "\n",
    "🧩 2️⃣ How to Fix It (Windows / Anaconda users)\n",
    "✅ Step 1 — Check your Keras cache location\n",
    "\n",
    "Open a terminal or Anaconda Prompt and run:\n",
    "\n",
    "echo %USERPROFILE%\\.keras\\datasets\n",
    "\n",
    "\n",
    "Then open that folder in File Explorer.\n",
    "\n",
    "You’ll likely see files like:\n",
    "\n",
    "imdb.npz\n",
    "imdb_word_index.json\n",
    "mnist.npz\n",
    "\n",
    "\n",
    "If the folder is too big or corrupted, delete old files:\n",
    "\n",
    "imdb.npz\n",
    "imdb_word_index.json\n",
    "\n",
    "\n",
    "Then rerun your notebook — Keras will automatically re-download clean copies.\n",
    "\n",
    "✅ Step 2 — Clean temporary folders\n",
    "\n",
    "Run this in Command Prompt (close Jupyter first):\n",
    "\n",
    "del /q/f/s %TEMP%\\*\n",
    "\n",
    "\n",
    "This removes temporary cache files that often eat space.\n",
    "\n",
    "✅ Step 3 — (Optional) Change Keras dataset storage location\n",
    "\n",
    "If your C: drive is low on space, you can make Keras use another folder, e.g. D:\\keras_datasets.\n",
    "\n",
    "Before loading the dataset, run:\n",
    "\n",
    "import os\n",
    "os.environ['KERAS_HOME'] = 'D:/keras_datasets'\n",
    "\n",
    "\n",
    "Then your datasets will be downloaded there instead of C:\\Users\\<name>\\.keras.\n",
    "\n",
    "✅ Step 4 — Verify space\n",
    "\n",
    "Make sure you have at least 500MB free on the drive where Anaconda and .keras are located.\n",
    "You can check by right-clicking your drive → Properties.\n",
    "\n",
    "🧩 3️⃣ After Cleaning\n",
    "\n",
    "Once you free space, rerun these lines:\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "\n",
    "✅ You should see it download quickly:\n",
    "\n",
    "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
    "\n",
    "\n",
    "and then continue normally.\n",
    "\n",
    "🧠 Quick Recap\n",
    "Issue\tMeaning\tFix\n",
    "OSError: [Errno 28] No space left on device\tDisk or cache folder full\tDelete old Keras datasets / free space\n",
    "Where data is stored\tC:\\Users\\<name>\\.keras\\datasets\tClean this folder\n",
    "Avoid future issues\tSet KERAS_HOME to another drive\tos.environ['KERAS_HOME'] = 'D:/keras_datasets'\n",
    "\n",
    "Would you like me to give you a one-line cleanup script you can safely run in Anaconda to automatically delete all old Keras datasets and free up space (without affecting your environment)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3c5f9-1b5e-4b42-8eed-bad7e3631cee",
   "metadata": {},
   "source": [
    "Line-by-line explanation — IMDb decoding & preview (complete, detailed)\n",
    "\n",
    "Below I’ll explain every line (and small groups of lines) from your IMDb preview + decoding + padding section. I’ll say what each line does, what the types/shapes are, why it’s there, and common pitfalls or alternatives. Read it as if I’m walking you through the notebook while pointing at each line.\n",
    "\n",
    "🧾 Section 1 — Load and Preview IMDb Dataset\n",
    "# Import libraries\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "Imports the built-in IMDb dataset loader from Keras. This gives you the function imdb.load_data() which returns tokenized review data (lists of integers) and labels.\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "Imports a utility that will convert lists of variable-length token sequences into fixed-length arrays by padding or truncating. You use this before feeding data to a neural network.\n",
    "\n",
    "# Set parameters\n",
    "vocab_size = 10000   # only keep top 10,000 words\n",
    "maxlen = 200         # cut or pad all reviews to 200 words\n",
    "\n",
    "\n",
    "vocab_size = 10000\n",
    "We'll only keep the top 10,000 most frequent words from the dataset. Any word with an index ≥ 10000 will be treated as \"out of vocabulary\" (mapped to the UNK or unknown token when loading with num_words).\n",
    "\n",
    "maxlen = 200\n",
    "All reviews will be forced to length 200 tokens: shorter reviews are padded (with zeros), longer ones are truncated. This produces arrays of shape (num_samples, 200) required by Keras models.\n",
    "\n",
    "# Load the IMDb dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "\n",
    "imdb.load_data(num_words=vocab_size)\n",
    "Downloads (if not cached) and loads the IMDb dataset. Returns two tuples: training and test sets.\n",
    "\n",
    "x_train — a Python list of len = 25000 where each element is a list of integers (word indices).\n",
    "\n",
    "y_train — a list/array of 25000 binary labels (0 or 1).\n",
    "\n",
    "Same for x_test, y_test (also 25000 samples).\n",
    "\n",
    "num_words=vocab_size tells Keras to only keep tokens with index < vocab_size. Anything beyond that will be excluded or replaced by reserved indices.\n",
    "\n",
    "Type / shape before padding:\n",
    "\n",
    "type(x_train) == list, len(x_train) == 25000\n",
    "\n",
    "type(x_train[0]) == list, len(x_train[0]) varies (e.g., 50–500)\n",
    "\n",
    "print(\"Training samples:\", len(x_train))\n",
    "print(\"Testing samples:\", len(x_test))\n",
    "print(\"Example tokenized review:\", x_train[0][:20])\n",
    "print(\"Label:\", y_train[0])\n",
    "\n",
    "\n",
    "len(x_train) / len(x_test) — print number of samples (should be 25000 each).\n",
    "\n",
    "x_train[0][:20] — print first 20 tokens (word indices) of the first review to show it’s numeric, not plain text.\n",
    "\n",
    "y_train[0] — print the label (0 or 1) for that first review.\n",
    "\n",
    "Why: quick sanity check: data loaded, labels exist, and tokens are integer indices.\n",
    "\n",
    "🔁 Section 2 — Decode Reviews into Readable Text\n",
    "\n",
    "We now convert integer tokens back into words so you can read the review.\n",
    "\n",
    "# Get the word index mapping\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "\n",
    "imdb.get_word_index() returns a dictionary mapping words → integer index.\n",
    "Example: {'the': 1, 'and': 2, ...} (indices are Keras’s internal mapping, but note we’ll shift them in the next step).\n",
    "\n",
    "Type: word_index is a dict with str keys and int values.\n",
    "\n",
    "# Adjust indices (Keras reserves first 3)\n",
    "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "\n",
    "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
    "Keras reserves indices 0–3 for special tokens. The original word_index maps words to indices starting at 1 — we shift them by +3 so we can insert special tokens at indices 0..3.\n",
    "\n",
    "word_index[\"<PAD>\"] = 0\n",
    "Reserve index 0 for padding. This makes pad_sequences use 0 as the padding value.\n",
    "\n",
    "word_index[\"<START>\"] = 1\n",
    "Index 1 marks the start of a sequence (some datasets insert a START token).\n",
    "\n",
    "word_index[\"<UNK>\"] = 2\n",
    "Index 2 used for unknown words (words excluded by num_words).\n",
    "\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "Reserved unused token (Keras historically included it).\n",
    "\n",
    "Why shift indices: Because imdb.load_data() uses indices 0.. for special reserved tokens; we must align our reverse map with those reserved IDs.\n",
    "\n",
    "Pitfall: If you skip the +3 shift you’ll decode wrong words (off-by-index errors).\n",
    "\n",
    "# Reverse the mapping to get words back\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "\n",
    "Creates reverse_word_index where keys are indices → values are words.\n",
    "Example: reverse_word_index[1] == \"<START>\", reverse_word_index[4] == \"the\", etc.\n",
    "\n",
    "Type: dict mapping int → str.\n",
    "\n",
    "# Function to decode reviews\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])\n",
    "\n",
    "\n",
    "def decode_review(encoded_review): defines a helper function.\n",
    "\n",
    "' '.join([...]) builds a single string from a list of tokens.\n",
    "\n",
    "reverse_word_index.get(i, '?') — for each token index i, return the corresponding word; if i not found, return '?'.\n",
    "\n",
    "Why: safer than direct indexing — handles unexpected indices.\n",
    "\n",
    "# Display a few decoded reviews\n",
    "for i in range(3):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Review {i+1}:\")\n",
    "    print(decode_review(x_train[i][:100]))\n",
    "    print(\"Sentiment:\", \"Positive\" if y_train[i] == 1 else \"Negative\")\n",
    "\n",
    "\n",
    "for i in range(3): — loop to print first 3 reviews.\n",
    "\n",
    "x_train[i][:100] — decode only the first 100 tokens to avoid giant prints.\n",
    "\n",
    "decode_review(...) — prints readable text approximating the original review.\n",
    "\n",
    "\"Positive\" if y_train[i] == 1 else \"Negative\" — map label 1 → Positive, 0 → Negative.\n",
    "\n",
    "Output: A readable snippet of reviews with labels.\n",
    "\n",
    "Note: The decoded text will include tokens like <START> or <UNK> for special/unknown tokens.\n",
    "\n",
    "🧩 Section 3 — Pad Sequences for Model Input\n",
    "# Pad all sequences to fixed length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(\"Padded training shape:\", x_train.shape)\n",
    "print(\"Padded test shape:\", x_test.shape)\n",
    "\n",
    "\n",
    "pad_sequences(x_train, maxlen=maxlen)\n",
    "Converts the list of variable-length sequences into a 2D NumPy array with shape (num_samples, maxlen):\n",
    "\n",
    "If a sequence is shorter than maxlen, it is padded with zeros (by default at the start; padding='post' changes this to the end).\n",
    "\n",
    "If longer, it is truncated (by default at the start; truncating='post' cuts the end instead).\n",
    "\n",
    "After these lines:\n",
    "\n",
    "x_train.shape typically (25000, 200)\n",
    "\n",
    "x_test.shape typically (25000, 200)\n",
    "\n",
    "Why padding: Neural networks process fixed-size tensors in batches. RNNs expect sequences to be same length per batch.\n",
    "\n",
    "Alternatives / options:\n",
    "\n",
    "pad_sequences(..., padding='post') — pads at the end (often desirable for RNNs so the start of sequence aligns at index 0).\n",
    "\n",
    "truncating='post' — prefer to keep the beginning of long reviews instead of the end.\n",
    "\n",
    "Use dtype='int32' if you want to ensure dtype.\n",
    "\n",
    "✅ Final notebook flow (what happens next)\n",
    "\n",
    "You will typically follow the preview/padding steps with:\n",
    "\n",
    "Train SimpleRNN — Embedding → SimpleRNN → Dense\n",
    "\n",
    "Good to illustrate conceptually, but often weaker performance.\n",
    "\n",
    "Train LSTM — Embedding → LSTM → Dense\n",
    "\n",
    "Handles long-term dependencies using gates (input, forget, output).\n",
    "\n",
    "Train GRU — Embedding → GRU → Dense\n",
    "\n",
    "Simpler than LSTM with comparable performance and fewer parameters.\n",
    "\n",
    "Compare validation/test accuracy to see differences.\n",
    "\n",
    "⚠️ Common pitfalls & tips\n",
    "\n",
    "Index shift errors: If you don’t shift word_index by +3 (or you use a different num_words), your reverse_word_index will be misaligned — decoded text will be gibberish.\n",
    "\n",
    "Padding direction matters: Default pad_sequences uses padding='pre'. For interpretability and some RNN behaviors you may prefer padding='post'.\n",
    "\n",
    "Token 0 is padding: After padding, 0 means “no word”. Ensure your Embedding layer does not learn meaningful embeddings for padding (Keras handles this well, but you can set mask_zero=True in Embedding to mask padded values for RNN layers that support masking).\n",
    "\n",
    "Memory & speed: Loading full dataset and training can be slow on CPU. Use GPU if available. Use fewer epochs for quick experiments (3–5), then increase.\n",
    "\n",
    "Unseen words: num_words=vocab_size removes rare words; when decoding you’ll see <UNK> for unknown tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c167aac-8843-41d8-9228-1d4494dd1256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
